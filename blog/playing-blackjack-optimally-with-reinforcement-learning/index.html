<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Meta tags-->
<!-- Give a small description in 3-4 sentences about the website -->
<meta name="description"
    content="IEEE NITK is a local chapter of the IEEE Organisation belonging in the Bangalore R10 Section. IEEE NITK undertakes lots of projects by its various SIGs which are formed and projects undertaken by students are futher applied to help solve daily world problems.">
<!-- A small abstract idea about the website -->
<meta name="abstract" content="This Website is about IEEE NITK Student Chapter">
<!-- Important Keywords to be noted in the website  -->
<meta name="keywords" content="IEEE, IEEE NITK, NITK Surathkal, NITK, Clubs in NITK, Technical Clubs in NITK">
<!-- Tell the spider to index the first page and other pages as well-->
<meta name="robots" content="index, follow">
<!-- How often should spiders come back to your page -->
<meta name="revisit-after" content="3 days">

<!-- Copyright regarding the website -->
<meta name="copyright" content="IEEE NITK">
<!-- Tells Google Bot not to duplicate description -->
<meta name="googlebot" content="noodp">
<!-- Language for the website -->
<meta name="language" content="English">

<!-- Web Author for the image -->
<meta name="web_author" content="IEEE NITK">
<meta name="author" content="Salman Shah">
<!-- Email ID -->
<meta name="contact" content="chairman.nitkieee@gmail.com" />
<!-- Email ID to reply to -->
<meta name="reply-to" content="chairman.nitkieee@gmail.com">

<!-- Refers to distribution of the page -->
<meta name="distribution" content="global">
<!-- Generator or formatter tag -->
<meta name="generator" content="Jekyll">
<!-- Disallow spammers for the webpage -->
<meta name="no-email-collection" content="http://www.metatags.info/nospamharvesting">
<!-- Rating for the page -->
<meta name="rating" content="general">
<!-- Content Type for the page -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<!-- Fixing viewport on mobile views -->
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0">

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Google Authorship Markup -->
<!-- Social: Twitter -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@IEEE_NITK">
<meta name="twitter:title" content="Playing Blackjack Optimally with Reinforcement Learning">
<meta name="twitter:description"
    content="An insight into Reinforcement Learning methods and how they work on blackjack">
<meta property="twitter:image:src"
    content="https://ieee.nitk.ac.in/blog/assets/img/blog-image.png">

<!-- Social: Facebook / Open Graph -->
<meta property="fb:app_id" content="0011038251882641" />
<meta property="og:url" content="https://ieee.nitk.ac.in/blog/playing-blackjack-optimally-with-reinforcement-learning/" />
<meta property="og:title" content="Playing Blackjack Optimally with Reinforcement Learning">
<meta property="og:image" content="https://ieee.nitk.ac.in/blog/assets/img/blog-image.png">
<meta property="og:description"
    content="An insight into Reinforcement Learning methods and how they work on blackjack">
<meta property="og:site_name" content="A blog about our findings and musings">

<!-- Social: Google+ / Schema.org  -->
<meta itemprop="name" content="Playing Blackjack Optimally with Reinforcement Learning" />
<meta itemprop="description"
    content="An insight into Reinforcement Learning methods and how they work on blackjack">
<meta itemprop="image" content="https://ieee.nitk.ac.in/blog/assets/img/blog-image.png" />
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<!-- Windows 8 Tile Icons -->
<meta name="application-name" content="IEEE-NITK Blog">
<meta name="msapplication-TileColor" content="#0562DC">
<meta name="msapplication-square70x70logo" content="smalltile.png" />
<meta name="msapplication-square150x150logo" content="mediumtile.png" />
<meta name="msapplication-wide310x150logo" content="widetile.png" />
<meta name="msapplication-square310x310logo" content="largetile.png" />

<!-- Android Lolipop Theme Color -->
<meta name="theme-color" content="#0562DC">

<!-- Linked Sheets needed -->
<link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Nunito:200,300,400" rel="stylesheet">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
    integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
<link rel="stylesheet" href="/blog/assets/css/icomoon.css">
<link rel="stylesheet" href="/blog/assets/css/animate.css">
<link rel="stylesheet" href="/blog/assets/css/bootstrap.css">
<link rel="stylesheet" href="/blog/assets/css/style.css">

<!-- Linked Scripts needed -->
<script src="https://preview.colorlib.com/theme/unapp/js/modernizr-2.6.2.min.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"
    integrity="sha512-CEiA+78TpP9KAIPzqBvxUv8hy41jyI3f2uHi7DGp/Y/Ka973qgSdybNegWFciqh6GrN2UePx2KkflnQUbUhNIA=="
    crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-countto/1.2.0/jquery.countTo.js"></script>

<!-- Favicon -->
<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon" />
<link rel="manifest" href="/blog/assets/img/icons/manifest.json">

<title>Blog | IEEE NITK</title>

<!-- highlighter theme  -->
<link rel="stylesheet" href="/blog/assets/css/monokai-sublime.css">

<link rel="stylesheet" href="/blog/assets/css/blog.css">
<link rel="canonical" href="/blog/playing-blackjack-optimally-with-reinforcement-learning/">
<link rel="alternate" type="application/rss+xml" title="A blog about our findings and musings"
    href="https://ieee.nitk.ac.in/blog/feed.xml" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- This property belongs to IEEE NITK -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L6M5H3BJC8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L6M5H3BJC8');
</script>

    <script src="/blog/assets/js/sidebar.js"></script>
    <script>
        loadArticles();
    </script>
</head>

<body>
    <div class="colorlib-loader">
        <div id="loader"></div>
        <div id="loader"></div>
        <div id="loader"></div>
        <div id="loader"></div>
        <div id="loader"></div>
    </div>
    <div id="page">
        <nav class="colorlib-nav" role="navigation">
    <div class="top-menu">
        <div class="container">
            <div class="row" id="header">
                <div class="col-md-2">
                    <div id="colorlib-logo">
                        <a href="https://ieee.nitk.ac.in" title="home">
                            <img src="/blog/assets/img/ieee_nitk.png" alt="IEEE Logo" class="navbar-logo">
                        </a>
                    </div>
                </div>
                <div class="col-md-10 text-right menu-1">
                    <ul>
                        
                            
                                <li><a href="https://ieee.nitk.ac.in/">Home</a></li>
                            
                        
                            
                                <li class="dropdown-nav">
                                    <a class="nav-link dropdown-toggle has-dropdown"  style="cursor: pointer;" id="navbarDropdownMenuLink" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        About
                                    </a>
                                    <ul class="dropdown dropdown-menu">
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/about-us.html">About Us</a></li>
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/team.html">Team</a></li>
                                    
                                    </ul>
                                </li>
                            
                        
                            
                                <li class="dropdown-nav">
                                    <a class="nav-link dropdown-toggle has-dropdown" style="cursor: pointer;" id="navbarDropdownMenuLink" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        SIGs
                                    </a>
                                    <ul class="dropdown dropdown-menu">
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/compsoc.html">Computer Society</a></li>
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/diode.html">Diode</a></li>
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/piston.html">Piston</a></li>
                                    
                                    </ul>
                                </li>
                            
                        
                            
                                <li class="dropdown-nav">
                                    <a class="nav-link dropdown-toggle has-dropdown"  style="cursor: pointer;" id="navbarDropdownMenuLink" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Affinity Groups
                                    </a>
                                    <ul class="dropdown dropdown-menu">
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/wie.html">Women in Engineering</a></li>
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/sight.html">SIGHT</a></li>
                                    
                                    </ul>
                                </li>
                            
                        
                            
                                <li><a href="https://ieee.nitk.ac.in/events.html">Events</a></li>
                            
                        
                            
                                <li><a href="https://ieee.nitk.ac.in/blog/">Blog</a></li>
                            
                        
                            
                                <li><a href="https://ieee.nitk.ac.in/gyan/">Gyan</a></li>
                            
                        
                            
                                <li class="dropdown-nav">
                                    <a class="nav-link dropdown-toggle has-dropdown" style="cursor: pointer;" id="navbarDropownMenuLink" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Achievements
                                    </a>
                                    <ul class="dropdown dropdown-menu">
                                        
                                            <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/achieve.html">Awards</a></li>
                                        
                                            <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/publications.html">Publications</a></li>
                                        
                                    </ul>
                                </li>
                            
                        
                            
                                <li class="dropdown-nav">
                                    <a class="nav-link dropdown-toggle has-dropdown"  style="cursor: pointer;" id="navbarDropdownMenuLink" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Virtual Expo
                                    </a>
                                    <ul class="dropdown dropdown-menu">
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/virtual-expo/compsoc/">CompSoc</a></li>
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/virtual-expo/diode/">Diode</a></li>
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/virtual-expo/piston/">Piston</a></li>
                                    
                                        <li class="dropdown-item"><a href="https://ieee.nitk.ac.in/virtual-expo/envision/">Envision</a></li>
                                    
                                    </ul>
                                </li>
                            
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
        <div class="nav-bg"></div>
<div class="container">
    <div class="header-post col-md-12 colorlib-bg-white animate-box" role="banner">
        <div class="content">
            <h1 class="post-title">Playing Blackjack Optimally with Reinforcement Learning</h1>
            <p class="author-small">
                <span class="author-img"
                    style="background-image: url(/blog/assets/img/authors/Shreya_Namath.jpg);"></span>
                <span class="author">by Shreya Namath</span>
                
                &nbsp;
                |
                &nbsp;
                <i class="fa fa-calendar"></i>
                <time itemprop="datePublished" datetime="2021-08-09 05:30:00 +0530"
                    class="header-date">09 Aug 2021</time>
                
            </p>
            <p class="subtitle">An insight into Reinforcement Learning methods and how they work on blackjack</p>
        </div>
    </div>
</div>
        <section class="post" itemscope itemtype="http://schema.org/BlogPosting">

            <div class="container">
                <div class="col-md-9 animate-box" style="padding: 0;">
                    <div class="colorlib-bg-white mt mr">
                        <article role="article" id="scroll" class="post-content" itemprop="articleBody">
                            <h2 id="introduction-to-reinforcement-learning">Introduction to Reinforcement Learning</h2>

<p>Reinforcement learning(RL) is a branch of machine learning which draws from <strong>the way humans learn to do tasks</strong>. For anyone unfamiliar with Pavlov, he is a physicist popularly known for his work in conditioned response. He trained a dog such that every time he rang a bell, he would immediately give the dog food. The dog soon learned to salivate at the sound of the bell. The same foundation to Jim’s mint prank in The Office for those who get the reference xD.</p>

<p><img src="/blog/assets/img/blackjack/pavlov.jpg" alt="Pavlov's Experiment" /></p>

<p>Reinforcement learning (or RL) takes Pavlov’s experiment one step further. Suppose the new experiment was that when Pavlov rang each bell out of a set of them, the dog would have to perform some corresponding trick to receive his treat. The dog, through trial and error, would eventually observe and learn the correct action to take for each bell to get as many treats as he can. This is essentially how dog-training and, you guessed it, RL in its basic form works!</p>

<p>For a quick insight into the technical basics of RL, <em>it is a setup where an agent (our dog) takes an action in some environment and then receives observations (the different ringing bells) and reward (yummy treats) from that environment</em>. The environment consists of <strong>states</strong> (in this case, what bell is ringing), and in each state, the agent takes an <strong>action</strong> to get to another state. While the next state and reward received are out of the agent’s control or full prior knowledge, what it can do is learn from those findings to take better actions. The fundamental objective is to maximize the cumulative expected reward, also called expected return. Depending on the factor γ, also called the discounting factor, the amount of importance given to future rewards may be reduced. γ decides how short-sighted or far-sighted the agent is, i.e: how much it prioritizes reward it receives in the near future compared to what it would receive later.</p>

<p><img src="/blog/assets/img/blackjack/rl.jpg" alt="RL basic idea" /></p>

<p>What is referred to as the agent’s <strong>policy</strong> can be thought of as a strategy while playing a game. More formally, it’s <strong>a function that maps a state to an action or a distribution over actions</strong>. We also define an <strong>action-value function</strong>, Q that gives us a measure of how good it is to take a particular action in each state, or to know how favorable each action is in a state of the game. There is also a <strong>state-value function</strong> which tells you how good it is to be in each state. It is the expected reward that is obtained by following a given policy, starting from that state.</p>

<p>All this sounds very similar to a human playing a game, and you’re right, it really is. The usual concept of points conveniently plays the role of reward. So, let’s understand it better by playing the casino card game of blackjack.</p>

<h2 id="blackjack-rules">Blackjack rules</h2>

<h3 id="here-are-the-basic-rules">Here are the basic rules</h3>

<p>Here all face cards have the value of 10. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), he wins unless the dealer also has the same, in which case the game is a draw. If the player does not have 21, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. <em>The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise</em>. If the dealer goes bust, then the player wins; otherwise, the outcome — win, lose, or draw — is determined by whose final sum is closer to 21. Note that in the game, an ace could count as either a 1 or 11, you generally count it as 11 unless your sum exceeds 21 in which you reduce it to 1 which prevents you from going bust. If the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable.</p>

<h2 id="model-free-methods">Model free methods</h2>

<p>Before we get into actually playing the game, more like watching the agent learn to play, bear with some more technical aspects that will give us a better understanding of what’s happening under the hood.</p>

<p>Here model-free essentially means that you, or rather, the agent doesn’t know the exact dynamics (state-transition probabilities) of the environment. What that means here is that you don’t have prior knowledge in Blackjack what the next state will be when you choose to hit or stay. If you did, then playing wouldn’t really be much of a task and casinos world-wide would go bankrupt. All you’d need to do is look through this knowledge to figure out what to do in each state to go to the most favorable next state to essentially find the optimal policy, one of the ways this can be done is through dynamic programming. But in blackjack, and most other interesting applications, you don’t know what could happen next so you need to play and figure it out with experience. To kick off this experience we follow some arbitrary policy, but one that lets us explore states so that we don’t overlook some advantageous ones that we may deem unfavorable  at first. This is often referred to as the <strong>exploration-exploitation dilemma</strong>. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. There are algorithms that deal with this. We accomplish this through ϵ-greedy policies, ones that will mostly take the optimal action and learn from what happens next, but with a probability of ϵ, will explore the consequences of a random action so as to learn more about each state eventually and ensure that we reach the global optimum solution.</p>

<p>Our task really comes down to two tasks we can repeat over and over again to reach our optimal policy: we should be able to evaluate the current policy we are following and secondly, use this evaluation to improve our current policy. Formally, this is known as Generalized Policy Iteration (GPI).</p>

<p><img src="/blog/assets/img/blackjack/GPI.png" alt="GPI" /></p>

<h2 id="lets-play">Let’s Play!</h2>

<h3 id="monte-carlo-method">Monte Carlo method</h3>

<p>The term Monte Carlo is often used more broadly for any estimation method that relies on repeated random sampling. In RL Monte Carlo methods allow us to estimate values directly from experience, from sequences of states, actions and rewards.
We use this as a way to evaluate the policy. Here we sample sequences of episodes: State, action, reward generated by following the policy till termination of the episode and the back propagate from the end to update the value of each state in the episode. The value of each state is the expected return from that state which can be calculated incrementally.</p>

<p>As an example, this is the evaluation of a policy that chooses to stick only on values of 20 or 21 (source: Sutton and Barto)</p>

<p><img src="/blog/assets/img/blackjack/mc-eval.jpg" alt="Monte Carlo evaluation" /></p>

<p>The game plan is to initialize our policy such that it’s stochastic which means that all actions have a possibility of being selected (here we just assign all probabilities to the same value or randomly initialize the policy). We evaluate the current policy using Monte Carlo evaluation and then we can easily improve this current policy but acting greedy with respect to it (we actually act ϵ greedily). Basically, look at the values corresponding to the current state in the Q-function and choose the action that maximizes the value. With this new policy, we re-compute the Q-table and the process goes on.
Mathematically, going in reverse from the time step of termination of the episode, the return for each state-action pair is computed according to the following formula, with future rewards being discounted by a factor of γ raised to the power of how much later they are received.
\(G_t=R_{t+1} + \gamma R_{t+2}+...+ \gamma ^{T-1}R_T\)
Now the values are updated according to :
\(Q(S,A)=Q(S,A)+ \alpha (G_t-Q(S,A))\)
So the value is moved alpha (learning rate) times the error between what was previously computed to be the expected return and the return we actually received. To compute the returns, the value must be computed in reverse from the end of each episode. The backup diagram looks something like this:</p>

<p><img src="/blog/assets/img/blackjack/mc-backup.png" alt="MC backup" /></p>

<p>The optimal policy learnt by this algorithm is as follows:</p>

<p><img src="/blog/assets/img/blackjack/pistar.png" alt="Optimal policy from Monte Carlo" />
(source: Sutton and Barto)</p>

<p>The agent we trained for 500,000 episodes with decaying epsilon value learnt the following policy:</p>

<p><img src="/blog/assets/img/blackjack/mc-pol.JPG" alt="Monte Carlo policy" /></p>

<p>At a glance, it seems in the case of a usable ace, our agent learns a policy very similar to the HIT17 policy of our dealer. Since we can’t outright judge the quality of the policy, we observe the player playing 1000 rounds against the dealer and note the statistics.</p>

<p>wins = 434.0<br />
draws = 89.0<br />
losses = 477.0<br /></p>

<p>However, we still can’t manage to beat the dealer but that isn’t too concerning here, since HIT17 is a pretty great policy to begin with and blackjack, like most casino games, is rigged to be in favor of the dealer. Regardless, if our agent learned  a policy even slightly better than HIT17, it would have had more wins than losses.</p>

<h4 id="drawbacks-of-mc">DRAWBACKS OF MC</h4>

<p>Monte Carlo methods inherently assume that the task you are using it on is episodic since it does wait till termination to update the Q-values. So, this algorithm can’t be applied to continuing tasks (for example, a personal assistant robot, its task doesn’t really have finite episodes and never ends). Additionally, say eventually on termination you got a big  negative reward; this would affect the Q-values of all the states that came before in the episode, so the credit or blame isn’t assigned properly to certain states. While this may be fine for short episodic tasks like blackjack, for longer-horizon ones, it would be better to use temporal difference methods. Of these methods, here we will discuss Q-learning.</p>

<p>Before moving on, do note that blackjack is one of the cases where Monte Carlo works well given the nature of the environment. The game is episodic, each episode is short and the reward only appears at the end of the episode.</p>

<h3 id="temporal-difference-methods">TEMPORAL DIFFERENCE METHODS</h3>

<p>Temporal Difference (TD) methods <strong>bootstrap</strong>. What that fancy term means is that at each time step, it updates the values of each state by looking at the value of the next state (TD(0)) and working its way back. There are 3 methods under this: sarsa, Q-learning and expected-sarsa, of which we will assess Q-learning here.
Here is a visual of what state or action values each method considers for backups</p>

<p><img src="/blog/assets/img/blackjack/backups.png" alt="backup diagrams" /></p>

<h3 id="q-learning">Q-learning</h3>

<p>Here we use a similar initialization for Q-values for each state and action. We sample episodes and at each time step, the agent chooses an action ϵ-greedily, observes the reward from the next state and backs up the value for the current state-action pair. In Q-learning, in each step of each episode we sample, after choosing the action A and observing the reward R and next state S’, the action-value corresponding to S,A is updated according to the formula:
\(Q(S,A)=Q(S,A)+ \alpha [R+ \gamma max(Q_a(S',a))-Q(S,A)]\)
The difference between this and our previous monte carlo update is in the target for update. Previously it was the sampled return which was calculated in reverse from episode termination. Here the target is the reward received + the discount factor times the maximum value from the next state, out of all the possible actions from that state. This way, for each update, what is considered is the maximum over the action values of the next state. The backup diagram looks like this:</p>

<p><img src="/blog/assets/img/blackjack/q-backup.png" alt="Q-learning backup" /></p>

<p>We once again train our agent on 500,000 episodes and it learns the following policy:</p>

<p><img src="/blog/assets/img/blackjack/q-pol.JPG" alt="Q-learning policy" /></p>

<p>After 1000 games based on the above policy:</p>

<p>wins = 420.0<br />
draws = 92.0<br />
losses = 488.0<br /></p>

<p>From these statistics, it does seem that our Q-learning policy didn’t perform as well as Monte Carlo; that could perhaps benefit from more rounds of training or tweaking the parameters.
Regardless, temporal difference methods are usually more efficient and converge quicker since it updates the Q-values at every step and not just when an episode ends.</p>

<h4 id="drawbacks-of-td">DRAWBACKS OF TD</h4>

<p>Temporal difference learning is faster but less stable and may converge to the wrong solution. We estimate the Q-values only from the current reward, and use our own estimate of the values of the next state to predict the future reward. While this is faster, our targets are based on our estimation itself, making it unstable. Moreover, our estimate starts off arbitrarily wrong, so we might train toward wrong targets and might converge to a wrong solution.</p>

<h2 id="what-next">What next?</h2>

<p>There’s much more to learn of reinforcement learning and here we have seen a bit of the basics and watched the algorithms learn to play a relatively simple game. Achievements like AlphaGo were accomplished through even more complex methods and also using the brilliant idea of letting the agent play against itself to get better (self-play), which has played an important part in beating human level performance.</p>

<p>There are other similar applications where things get more complex, for instance, the number of states itself could be massive and computing values is infeasible. There are many methods to deal with this and various other situations. A lot of the work done in RL is relatively recent and much research is still on-going into the possibilities of its application.</p>

<p>There’s loads to explore (and exploit :p). <strong>Here are some resources to get you started</strong>:</p>

<ul>
  <li><a href="http://incompleteideas.net/book/the-book-2nd.html">Book by Sutton and Barto</a></li>
  <li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A blog that covers the methods mentioned here</a></li>
</ul>

                        </article>
                    </div>
                </div>
                <div class="col-md-3 animate-box" style="padding: 0;">
                    <div class="colorlib-bg-white mt">
                        <div class="sidebar">
    <h2>Related Articles</h2>
    <div class="temp-div">
        <span id="category">compsoc</span>
        <span id="title">Playing Blackjack Optimally with Reinforcement Learning</span>
    </div>
    <div class="related-articles"></div>
</div>

                    </div>
                    <div class="colorlib-bg-white mt">
                        <div class="sidebar">
                            <h3>Browse IEEE Blog</h3>
                            <p>
                                <a href="/blog/series/">Series</a>
                                <br>
                                <a href="/blog/tags/">Tags</a>
                            </p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="container">
    <div class="colorlib-bg-white animate-box">
        <section class="author-big" itemprop="author">
            <div class="details" itemscope itemtype="http://schema.org/Person">
                <div class="col-md-4 col-sm-4 author-img-center">
                    <img itemprop="image" class="author-img-big"
                        src="/blog/assets/img/authors/Shreya_Namath.jpg"
                        alt="Shreya Namath">
                </div>
                <div class="col-md-8 sol-sm-8 author-details">
                    <p>Author</p>
                    <h3>
                        <strong>
                            Shreya Namath
                        </strong>
                    </h3>
                    <p class="desc"></p>
                    
                    <i class="fa fa-envelope"></i>
                    <a class="author-big-email" target="_blank"
                        href="mailto:shreyanamath033@gmail.com">shreyanamath033@gmail.com</a>
                    <br>
                    
                    <i class="fa fa-github"></i>
                    <a class="author-big-email" target="_blank"
                        href="https://github.com/Shreya301">Shreya301</a>
                </div>
            </div>
        </section>
    </div>
</div>

            <!-- Footer -->
<footer id="colorlib-footer">
    <div class="container">
        <div class="row footer-row">
            <div class="col-md-3 colorlib-widget">
                <h4>About IEEE</h4>
                <p>A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated 
                    to advancing technology for the benefit of humanity.
                </p>
                <p>
                    <img src="/blog/assets/img/ieee_white.png" class="footer-logo" alt="IEEE">
                    <img src="/blog/assets/img/ieee_nitk_white.png" class="footer-logo" alt="IEEE-NITK">
                </p>
            </div>
            <div class="col-md-3 colorlib-widget">
                <h4 class="secondary">Information</h4>
                <p>
                    <ul class="colorlib-footer-links">
                        <li><a href="https://ieee.nitk.ac.in"><i class="fa fa-check"></i> Home</a></li>
                        <li><a href="https://ieee.nitk.ac.in/team.html"><i class="fa fa-check"></i> Team</a></li>
                        <li><a href="/blog/"><i class="fa fa-check"></i> Blog</a></li>
                        <li><a href="https://ieee.nitk.ac.in/gyan/"><i class="fa fa-check"></i> Gyan</a></li>
                        <li><a href="https://ieee.nitk.ac.in/achieve.html"><i class="fa fa-check"></i> Achievements</a></li>
                        <li><a href="https://ieee.nitk.ac.in/publications.html"><i class="fa fa-check"></i> Publications</a></li>
                        <li><a href="https://ieee.nitk.ac.in/joinus.html"><i class="fa fa-check"></i> Join Us</a></li>
                    </ul>
                </p>
            </div>
            <div class="col-md-3 colorlib-widget">
                <h4 class="secondary">SIGs</h4>
                <div class="f-blog">
                    <div class="desc">
                        <h2><a href="https://ieee.nitk.ac.in/compsoc.html"><i class="fa fa-laptop"></i> &nbsp; Compsoc</a></h2>
                    </div>
                </div>
                <div class="f-blog">
                    <div class="desc">
                        <h2><a href="https://ieee.nitk.ac.in/diode.html"><i class="fa fa-plug"></i> &nbsp; Diode</a></h2>
                    </div>
                </div>
                <div class="f-blog">
                    <div class="desc">
                        <h2><a href="https://ieee.nitk.ac.in/piston.html"><i class="fa fa-cogs"></i> &nbsp; Piston</a></h2>
                    </div>
                </div>
                <br>
                <h4 class="secondary">Affinity Groups</h4>
                <div class="f-blog">
                    <div class="desc">
                        <h2><a href="https://ieee.nitk.ac.in/wie.html"><i class="fa fa-female"></i> &nbsp; Women in Engineering</a></h2>
                    </div>
                </div>
                <div class="f-blog">
                    <div class="desc">
                        <h2><a href="https://ieee.nitk.ac.in/sight.html"><i class="fa fa-users"></i> &nbsp; SIGHT</a></h2>
                    </div>
                </div>
                
            </div>
            <div class="col-md-3 colorlib-widget">
                <h4 class="secondary">Contact Info</h4>
                <ul class="colorlib-footer-links">
                    <li><strong>NITK Surathkal</strong><br>NH 66, Srinivasnagar<br>Surathkal, Mangalore<br>Karnataka 575025</li>
                    <li><a href="mailto:ieee@nitk.edu.in" target="_blank"><i class="fa fa-envelope"></i>ieee@nitk.edu.in</a></li>
                </ul>
                <p>
                    <ul class="colorlib-social-icons">
                        <li><a href="https://github.com/IEEE-NITK" target="_blank"><i class="fa fa-github"></i></a></li>
                        <li><a href="https://www.instagram.com/ieee_nitk" target="_blank"><i class="fa fa-instagram"></i></a></li>
                        <li><a href="https://twitter.com/IEEE_NITK" target="_blank"><i class="fa fa-twitter"></i></a></li>
                        <li><a href="https://facebook.com/IEEENITK" target="_blank"><i class="fa fa-facebook"></i></a></li>
                        <li><a href="https://www.linkedin.com/company/ieee-nitk-surathkal" target="_blank"><i class="fa fa-linkedin" target="_blank"></i></a></li>
                        <li><a href="/blog/sitemap.xml" target="_blank"><i class="fa fa-rss" target="_blank"></i></a></li>
                    </ul>
                </p>
            </div>
        </div>
    </div>
    <div class="copy">
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center">
                <p>
                    Copyright &copy;
                    <script>document.write(new Date().getFullYear());</script>
                    All rights reserved | Made with <i class="fa fa-heart" aria-hidden="true"></i> 
                    by IEEE-NITK, <a href="http://jekyllrb.com/" target="_blank" class="copyright-link">Jekyll</a> 
                    and <a href="https://colorlib.com" target="_blank" class="copyright-link">Colorlib</a>
                    <br>
                </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<div class="gototop js-top">
    <a href="" class="js-gotop"><i class="fa fa-arrow-up"></i></a>
</div>
            
<script>
	$(document).ready(function(){
		if($("pre code").length){
			
			$.getScript('../assets/js/highlight.pack.js', function(){
    			// load the highlighter script only if we have code snippets.
    			$('pre code').each(function(i, block) {
    				hljs.highlightBlock(block);
  				});	
    		});
		}
	});
</script>
        </section>
    </div>
    <script src="/blog/assets/js/main.js"></script>
</body>

</html>